{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook loads the training data, builds the model and evalutes the preformance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRR2c-tXloJS",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Setup\n",
    "\n",
    "Installs/Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Obtaining dependency information for opencv-python from https://files.pythonhosted.org/packages/d9/64/7fdfb9386511cd6805451e012c537073a79a958a58795c4e602e538c388c/opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.11/site-packages (from opencv-python) (1.24.3)\n",
      "Using cached opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.2 MB)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.9.0.80\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "iIasJF0PloJT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-13 18:53:23.979305: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-13 18:53:24.213198: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-13 18:53:25.198853: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Read In Image data\n",
    "\n",
    "Create data loader function and load image data into data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### read_data() definition\n",
    "\n",
    "*purpose*: to load data in from directory folder and organise metadata\n",
    "\n",
    "*input/output*: takes in list of names of folders containing image data, reads the image file, resizes the image to 224x224 pixel, converts the image data to a NumPy array and appends it to the data lis \n",
    "Appends the labesubfolder name), object ID (extracted from the image file name), sublabel (part of the image file name), and age (also from the image file naes) and appends it to each respective lst.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(folder):\n",
    "    data, labels, objectid, age, sublabel = [], [], [], [], []\n",
    "    for label in folder:\n",
    "        path = f\"{label}/\"\n",
    "        folder_data = os.listdir(path)\n",
    "        for image_path in folder_data:\n",
    "            if image_path.endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff')):\n",
    "                #print(path + image_path)\n",
    "                image_path_list = image_path.split('_')\n",
    "                img = cv2.imread(path + image_path, cv2.IMREAD_UNCHANGED)\n",
    "                img = cv2.resize(img, (224, 224))\n",
    "                data.append(np.array(img))\n",
    "                labels.append(label)\n",
    "                objectid.append(int(image_path_list[0]))\n",
    "                sublabel.append(image_path_list[1])\n",
    "                age.append(image_path_list[2].split('.')[0])\n",
    "\n",
    "    return data, labels, objectid, age, sublabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder = ['even', 'uneven'] #folders my images are split into\n",
    "data, labels,  objectid, age, sublabel = read_data(folder) #load data into %notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metadata data frame creation\n",
    "metadata = pd.DataFrame(\n",
    "    {\n",
    "        'labels': labels, # dependent variable, Y\n",
    "        'objectid': objectid, #unique row label from original data set\n",
    "        'age': age, #label indicating image is from before or after\n",
    "        'sublabel': sublabel #silvi_1 label \n",
    "    }\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Stacking images\n",
    "\n",
    "Create functions to assist in stacking task and consolidate metadata with stacked images' infomation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### index_finder()\n",
    "*purpose*: find index matching criteria provided\n",
    "\n",
    "*input/output*: Given object id and age return corrisoponding index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_finder(objectid, age):\n",
    "    return metadata.loc[(metadata.objectid == objectid) & (metadata.age == age)].index[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### combined_image()\n",
    "*purpose*: to stack image bands of the before and after versions of the same object id\n",
    "\n",
    "*input/output* given object id, find before and after images in folders and stack the bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_image(objectid):\n",
    "    return np.concatenate([data[index_finder(objectid, 'before')], data[index_finder(objectid, 'after')]],axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty lists for stacked images metadata\n",
    "combined_sublabel = []\n",
    "combined_labels = []\n",
    "combined_objectid = []\n",
    "combined_data = []\n",
    "\n",
    "objectid_set = set(objectid) # list of unique object ids\n",
    "\n",
    "#loop to stack all images\n",
    "for objectid in objectid_set: \n",
    "    #combine image\n",
    "    combined = combine_image(objectid)\n",
    "    #do not keep duplicates, the following if not keeps duplicates from being added\n",
    "    if not np.all(combined[:,:,:3] == combined[:,:,3:]):\n",
    "        combined_objectid.append(objectid)\n",
    "        index = index_finder(objectid, 'after')\n",
    "        combined_sublabel.append(sublabel[index])\n",
    "        combined_labels.append(labels[index])\n",
    "        combined_data.append(combine_image(objectid))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_data) #any changes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Train/Test Split\n",
    "\n",
    "Create new dataframe and split data into train adn test groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size:  1357\n",
      "Test Size:  340\n"
     ]
    }
   ],
   "source": [
    "#Combined/stacked images data frame (duplicate before/after images removed)\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'data': combined_data, \n",
    "        'label': combined_labels, \n",
    "        'objectid': combined_objectid\n",
    "    }\n",
    ")\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=12)\n",
    "\n",
    "print(\"Train Size: \", train.shape[0])\n",
    "print(\"Test Size: \", test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFqNNZywloJV"
   },
   "source": [
    "## Prepare the data\n",
    "\n",
    "Put infomarion into correct format to load into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (1357, 224, 224, 6)\n",
      "1357 train samples\n",
      "340 test samples\n",
      "(1357, 1)\n",
      "(1357, 224, 224, 6, 1)\n"
     ]
    }
   ],
   "source": [
    "# Model data parameters\n",
    "num_classes = 2\n",
    "input_shape = (224, 224, 6)\n",
    "# Load the data and split it between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = (train.data, train.label), (test.data, test.label)\n",
    "\n",
    "# Extract values from DataFrame\n",
    "x_train_values = np.array([np.array(img) for img in x_train])\n",
    "x_test_values = np.array([np.array(img) for img in x_test])\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train_values = x_train_values.astype(\"float32\") / 255\n",
    "x_test_values = x_test_values.astype(\"float32\") / 255\n",
    "\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "print(\"x_train shape:\", x_train_values.shape)\n",
    "print(x_train_values.shape[0], \"train samples\")\n",
    "print(x_test_values.shape[0], \"test samples\")\n",
    "\n",
    "# Make sure images have shape (224, 224, 6)\n",
    "x_train_values = np.expand_dims(x_train_values, -1)\n",
    "x_test_values = np.expand_dims(x_test_values, -1)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit label encoder and transform labels\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train_encoded, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test_encoded, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQdbLpH9loJW"
   },
   "source": [
    "## Build the model\n",
    "\n",
    "The model consists of three pairs of convolutional and max pooling layers, followed by a flattening layer, a dropout layer for regularization, and a dense output layer for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "B67H5SxEloJX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 222, 222, 32)      1760      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 111, 111, 32)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 109, 109, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 54, 54, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 52, 52, 32)        18464     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 26, 26, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 21632)             0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 21632)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 43266     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 81986 (320.26 KB)\n",
      "Trainable params: 81986 (320.26 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-13 20:23:32.601628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 43348 MB memory:  -> device: 0, name: NVIDIA L40, pci bus id: 0000:e1:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential( #initialize sequential model\n",
    "    [\n",
    "        keras.Input(shape=input_shape), #set input layer sixe (224, 224, 6)\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"), #adds a 2D convolutional layer w/ 32 kernels, each of size 3x3\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)), #adds a max pooling layer with a pool size of 2x2, makes next layers input smaller \n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"), #adds a 2D convolutional layer w/ 64 kernels, each of size 3x3\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"), \n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(), #transforms the input data into a 1D array\n",
    "        layers.Dropout(0.5), #adds dropout layer w/ rate = .5\n",
    "        #(Dropout: regularization technique to prevent overfitting by randomly setting a fraction of input units to 0 during training.)\n",
    "        layers.Dense(num_classes, activation=\"softmax\"), #adds a fully connected (dense) layer with num_classes neurons\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No9v_r8ploJX",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Train the model\n",
    "Set batch size and number of epochs and fit model with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "0HX-_OmQloJX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "39/39 [==============================] - 3s 24ms/step - loss: 0.6415 - accuracy: 0.6568 - val_loss: 0.4262 - val_accuracy: 0.8088\n",
      "Epoch 2/15\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.3997 - accuracy: 0.8313 - val_loss: 0.3845 - val_accuracy: 0.8235\n",
      "Epoch 3/15\n",
      "39/39 [==============================] - 1s 28ms/step - loss: 0.3843 - accuracy: 0.8387 - val_loss: 0.5532 - val_accuracy: 0.7426\n",
      "Epoch 4/15\n",
      "39/39 [==============================] - 1s 29ms/step - loss: 0.3821 - accuracy: 0.8436 - val_loss: 0.3581 - val_accuracy: 0.8529\n",
      "Epoch 5/15\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.3207 - accuracy: 0.8600 - val_loss: 0.3775 - val_accuracy: 0.8088\n",
      "Epoch 6/15\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.3038 - accuracy: 0.8755 - val_loss: 0.3430 - val_accuracy: 0.8456\n",
      "Epoch 7/15\n",
      "39/39 [==============================] - 1s 22ms/step - loss: 0.2804 - accuracy: 0.8829 - val_loss: 0.3374 - val_accuracy: 0.8676\n",
      "Epoch 8/15\n",
      "39/39 [==============================] - 1s 20ms/step - loss: 0.3073 - accuracy: 0.8706 - val_loss: 0.3710 - val_accuracy: 0.8309\n",
      "Epoch 9/15\n",
      "39/39 [==============================] - 1s 20ms/step - loss: 0.2519 - accuracy: 0.8968 - val_loss: 0.3638 - val_accuracy: 0.8382\n",
      "Epoch 10/15\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.2208 - accuracy: 0.9206 - val_loss: 0.3498 - val_accuracy: 0.8456\n",
      "Epoch 11/15\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.2120 - accuracy: 0.9197 - val_loss: 0.3510 - val_accuracy: 0.8529\n",
      "Epoch 12/15\n",
      "39/39 [==============================] - 1s 20ms/step - loss: 0.1732 - accuracy: 0.9361 - val_loss: 0.4031 - val_accuracy: 0.8382\n",
      "Epoch 13/15\n",
      "39/39 [==============================] - 1s 19ms/step - loss: 0.1532 - accuracy: 0.9451 - val_loss: 0.6930 - val_accuracy: 0.8015\n",
      "Epoch 14/15\n",
      "39/39 [==============================] - 1s 20ms/step - loss: 0.2119 - accuracy: 0.9115 - val_loss: 0.4569 - val_accuracy: 0.8309\n",
      "Epoch 15/15\n",
      "39/39 [==============================] - 1s 16ms/step - loss: 0.1178 - accuracy: 0.9615 - val_loss: 0.4666 - val_accuracy: 0.8382\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f9f0279cad0>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 15\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train_values, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eflofRIPloJY"
   },
   "source": [
    "## Evaluate the trained model\n",
    "\n",
    "Use test data to gauge models preformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "ix9bqorUloJY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.4968973994255066\n",
      "Test accuracy: 0.8441176414489746\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test_values, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0]) #lower the better\n",
    "print(\"Test accuracy:\", score[1]) #higher the better, less than .5 accracy is worse than just guessing"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "mnist_convnet",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
